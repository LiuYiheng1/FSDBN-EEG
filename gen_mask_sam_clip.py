import os
import torch
import numpy as np
from PIL import Image
from torchvision.transforms import Resize
import torch.nn.functional as F
from tqdm import tqdm

# ====== 1. å¯¼å…¥ MobileSAM ======
from MobileSAM.mobile_sam import sam_model_registry, SamAutomaticMaskGenerator

# ====== 2. å¯¼å…¥ CLIP ======
from clipmain.clipset import clip

# ====== é…ç½®è·¯å¾„ ======
IMG_ROOT = r"D:\pycharmproject\Uncertainty-aware-Blur-Prior-main\data\things-eeg\Image_set"
MASK_DIR = "weights/masks_sam_clip"  # æ–°ç›®å½•ï¼Œé¿å…è¦†ç›–æ—§æ©ç 
os.makedirs(MASK_DIR, exist_ok=True)

# ====== åŠ è½½æ¨¡å‹ ======
print("ğŸš€ åŠ è½½ MobileSAM...")
sam = sam_model_registry["vit_t"](checkpoint="MobileSAM/weights/mobile_sam.pt").eval().cuda()
mask_generator = SamAutomaticMaskGenerator(sam)

print("ğŸ–¼ï¸ åŠ è½½ CLIP (ViT-B/32)...")
clip_model, clip_preprocess = clip.load("ViT-B/32", device="cuda")
clip_model.eval()
for p in clip_model.parameters():
    p.requires_grad = False

resize_224 = Resize((224, 224), antialias=True)

def mask_to_bbox(mask):
    coords = np.argwhere(mask)
    if coords.size == 0:
        return None
    y_min, x_min = coords.min(axis=0)
    y_max, x_max = coords.max(axis=0)
    return (int(x_min), int(y_min), int(x_max), int(y_max))

def generate_foreground_mask(img_path):
    pil_img = Image.open(img_path).convert("RGB")
    image_np = np.array(pil_img)

    # Step 1: è·å–æ•´å›¾ CLIP ç‰¹å¾
    try:
        full_input = clip_preprocess(pil_img).unsqueeze(0).cuda()
        with torch.no_grad():
            full_feat = clip_model.encode_image(full_input).float()  # [1, 512]
    except Exception as e:
        print(f"âš ï¸ CLIP æ•´å›¾ç‰¹å¾å¤±è´¥: {e}")
        full_feat = None

    # Step 2: SAM åˆ†å‰²
    masks = []
    try:
        masks = mask_generator.generate(image_np)
    except Exception as e:
        print(f"âš ï¸ SAM åˆ†å‰²å¤±è´¥: {e}")
        masks = []

    best_mask = None
    best_score = -1.0

    # Step 3: å¦‚æœæœ‰æ•´å›¾ç‰¹å¾ï¼Œç”¨ CLIP æ‰“åˆ†é€‰æœ€ä½³ mask
    if full_feat is not None and masks:
        for m in masks:
            bbox = mask_to_bbox(m['segmentation'])
            if bbox is None:
                continue
            try:
                cropped = pil_img.crop(bbox)
                crop_input = clip_preprocess(cropped).unsqueeze(0).cuda()
                with torch.no_grad():
                    crop_feat = clip_model.encode_image(crop_input).float()
                sim = F.cosine_similarity(full_feat, crop_feat).item()
                if sim > best_score:
                    best_score = sim
                    best_mask = m['segmentation']
            except Exception:
                continue  # è·³è¿‡æ— æ•ˆ crop

    # Step 4: å¦‚æœæ²¡é€‰å‡º best_maskï¼Œç”¨æœ€å¤§é¢ç§¯ fallback
    if best_mask is None and masks:
        largest = max(masks, key=lambda x: x['area'])
        best_mask = largest['segmentation']

    # Step 5: å¦‚æœ still Noneï¼Œç”¨ center crop
    if best_mask is None:
        w, h = pil_img.size
        mask = np.zeros((h, w), dtype=np.float32)
        crop_w, crop_h = int(w * 0.6), int(h * 0.6)
        x1 = (w - crop_w) // 2
        y1 = (h - crop_h) // 2
        mask[y1:y1+crop_h, x1:x1+crop_w] = 1.0
        best_mask = mask

    # Step 6: è½¬ä¸º tensor å¹¶ resize åˆ° 224x224
    mask_tensor = torch.from_numpy(best_mask).float()
    if mask_tensor.ndim == 2:
        mask_tensor = resize_224(mask_tensor.unsqueeze(0)).squeeze(0)
    else:
        mask_tensor = resize_224(mask_tensor)

    return mask_tensor

# ====== ä¸»å¾ªç¯ ======
total_masks = 0

for split in ["training_images", "test_images"]:
    split_path = os.path.join(IMG_ROOT, split)
    if not os.path.exists(split_path):
        print(f"âš ï¸ è·¯å¾„ä¸å­˜åœ¨: {split_path}")
        continue

    class_dirs = [d for d in os.listdir(split_path) if os.path.isdir(os.path.join(split_path, d))]
    print(f"ğŸ“ {split}: å…± {len(class_dirs)} ä¸ªç±»åˆ«")

    for class_name in tqdm(class_dirs, desc=f"å¤„ç† {split}"):
        class_path = os.path.join(split_path, class_name)
        for img_file in os.listdir(class_path):
            if not img_file.lower().endswith('.jpg'):
                continue

            img_path = os.path.join(class_path, img_file)
            mask_name = os.path.splitext(img_file)[0] + ".pt"
            mask_save_path = os.path.join(MASK_DIR, mask_name)

            try:
                mask_f = generate_foreground_mask(img_path)
                torch.save(mask_f, mask_save_path)
                total_masks += 1
            except Exception as e:
                print(f"âŒ å®Œå…¨å¤±è´¥: {img_path} | {e}")

print(f"âœ… å…¨éƒ¨å®Œæˆï¼å…±ç”Ÿæˆ {total_masks} ä¸ªé«˜è´¨é‡å‰æ™¯æ©ç ã€‚")


